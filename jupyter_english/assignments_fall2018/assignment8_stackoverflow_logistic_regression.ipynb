{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\" />\n",
    "    \n",
    "## [mlcourse.ai](https://mlcourse.ai) – Open Machine Learning Course \n",
    "Authors: [Pavel Nesterov](http://pavelnesterov.info/) (@mephistopheies), [Yury Kashnitskiy](https://yorko.github.io) (@yorko), and [Daniel Potapov](https://www.linkedin.com/in/daniel-potapov/) (@sharthZ23). Edited by [Anastasia Manokhina](https://www.linkedin.com/in/anastasiamanokhina/) (@manokhina). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Assignment #8. Fall 2018\n",
    "## <center> StackOverflow questions tagging with logistic regression\n",
    "\n",
    "**You need to derive formulas where it's asked (yes, with pen and paper), fill in the code in the cells and select answers in the [web form](https://docs.google.com/forms/d/1FsawLzl_i3nw-ahECsJAJkd0SxFsmWDS5sS2hr6RbKY).**\n",
    "\n",
    "## 0. Problem description\n",
    "\n",
    "In this assignment, we will code a model for predicting tags based on a multilabel logistic regression. Unlike the usual setting of a multiclass problem, in this case one example can belong to several classes. We will implement an online version of the multilabel classification algorithm.\n",
    "\n",
    "We will use a small sample of 70k questions extracted from StackOverflow (about 23 MB if zipped, download from [here](https://drive.google.com/file/d/1djuygR3cTXb4go_KMWSOw94WlFKb2NT5/view?usp=sharing)).\n",
    "\n",
    "Actually, such implementations are used in real life (though not implemented in Python). For example, in online [Click-Through Rate](https://en.wikipedia.org/wiki/Click-through_rate) prediction models, the user is shown a banner, then, depending on the presence of a click, the model parameters are updated. In real applications, the amount of model parameters of can reach hundreds of millions, while one user usually has only a hundred or a thousand non-zero parameters out of this hundred millions, therefore it is not effective to vectorize such a model. Usually all user data is stored in huge clusters in in-memory databases, and user processing is distributed.\n",
    "\n",
    "> Data Science is the math for your business \n",
    "\n",
    "To perfectly grasp this folk wisdom, we'll dive into the math of multiclass & multilabel logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multiclass & multilabel logistic regression\n",
    "\n",
    "### 1.1. Softmax classifier (multiclass logistic regression)\n",
    "\n",
    "Let's see how logistic regression is derived for two classes $ \\left\\{0, 1\\right\\}$: the probability that an instance belongs to class $1$ is derived from the Bayes theorem:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = 1 \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x}\\right)} \\\\\n",
    "&=& \\dfrac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right) + p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} \\\\\n",
    "&=& \\dfrac{1}{1 + e^{-a}} = \\sigma\\left(a\\right)\n",
    "\\end{array}$$\n",
    "where:\n",
    "- $\\vec{x}$ – is a feature vector\n",
    "- $\\sigma$ – stands for the sigmoid function of a scalar argument\n",
    "- $a = \\log \\frac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} = \\sum_{i=0}^{d} w_i x_i$ – this relation is modeled by a linear function of features and model parameters\n",
    "\n",
    "This expression can easily be generalized to a set of $ K $ classes, only the denominator in the Bayes formula changes. Let us write down the probability that an instance belongs to class $ k $:\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = k \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right)}{\\sum_{i=1}^K p\\left(\\vec{x} \\mid c = i\\right)p\\left(c = i\\right)} = \\dfrac{e^{z_k}}{\\sum_{i=1}^{K}e^{z_i}} = \\sigma_k\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "where\n",
    "- $\\sigma_k$ – stands for a softmax function of a vector argument\n",
    "- $z_k = \\log p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right) = \\sum_{i=0}^{d} w_{ki} x_i$ – this relation is modeled by a linear function of features and model parameters for the class $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model the full likelihood of **one example**  (look, it's not the same as likelihood of the whole training set), we use the [categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution), or, to be more precise, its logarithm (for convenience):\n",
    "\n",
    "$$\\Large \\mathcal{L} = \\log p\\left({\\vec{x}}\\right) = \\log \\prod_{i=1}^K \\sigma_i\\left(\\vec{z}\\right)^{y_i} = \\sum_{i=1}^K y_i \\log \\sigma_i\\left(\\vec{z}\\right), $$\n",
    "\n",
    "where \n",
    "\n",
    "- $K$ is the number of classes\n",
    "- $y_i$ is either 0 or 1, depending on the true class label of the example $\\vec{x}$\n",
    "\n",
    "It turns out to be a famous [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) function (if multiplied by $-1$). Likelihood needs to be maximized, and, accordingly, cross entropy should be minimized. By differentiating with respect to the parameters of the model, we will obtain the rules for updating the weights for gradient descent, **do this derivation on your own**, you will need to understand this for further fulfillment of the task:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} &=& x_m \\left(y_k - \\sigma_k\\left(\\vec{z}\\right)\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Softmax classifier is very well [explained](http://cs231n.github.io/linear-classify/) in Stanford's course cs231n \"Convolutional Neural Networks for Visual Recognition\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Multilabel logistic regression\n",
    "\n",
    "It turns out that the softmax classifier tends to predict a high probability for some class and low probabilities for all other classes. That's due to an exponent in the formula of softmax. Also, in the previous formulation, it turns out that the vector $\\left(\\sigma_1, \\sigma_2, \\ldots, \\sigma_K\\right)$ forms a discrete probability distribution, i.e. $\\sum_{i=1}^K \\sigma_i = 1$. But in our problem statement each example can have several tags or can simultaneously belong to several classes. To take it into account we will slightly change the model:\n",
    "- We assume that all tags are independent of each other, i.e. each outcome is a logistic regression on two classes (either there is a tag or not), then the probability that an example has a tag will be written as following (each tag/class has its own set of parameters as in the case of a softmax classifier):\n",
    "$$\\large p\\left(\\text{tag}_k \\mid \\vec{x}\\right) = \\sigma\\left(z_k\\right) = \\sigma\\left(\\sum_{i=0}^d w_{ki} x_i \\right)$$\n",
    "- The presence of each tag will be modeled using <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\">Bernoulli distribution</a>\n",
    "\n",
    "Your first task is to write a simplified expression for the negative log-likelihood (NLL) of one training example. As a rule, many optimization algorithms have an interface for minimizing the function, and we follow the same tradition and multiply the resulting expression for log-likelihood by $-1$ to get NLL $-\\mathcal{L}$. In the second part,  we derive formulas to minimize the resulting expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">Question 1:</font> What's the correct formula for negative log-likelihood of one training example?**\n",
    "\n",
    "*For discussions, please stick to [ODS Slack](https://opendatascience.slack.com/), channel #mlcourse_ai, pinned thread __#a8_q1__*\n",
    "\n",
    "**<font color=\"red\">Answer options:</font>**\n",
    "1. $\\large -\\mathcal{L} = -\\sum_{i=1}^d y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "\n",
    "2. $\\large -\\mathcal{L} = -\\sum_{i=1}^d z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$\n",
    "\n",
    "3. $\\large -\\mathcal{L} = -\\sum_{i=1}^K z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$\n",
    "\n",
    "4. $\\large -\\mathcal{L} = -\\sum_{i=1}^K y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deriving the formula for weight updates\n",
    "\n",
    "In the second task, you need to derive the formula for the partial derivative of $-\\mathcal{L}$ w.r.t weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**<font color=\"red\">Question 2:</font> What's the correct formula for the derivative of negative log-likelihood w.r.t. to weights?**\n",
    "\n",
    "*For discussions, please stick to [ODS Slack](https://opendatascience.slack.com/), channel #mlcourse_ai, pinned thread __#a8_q2__*\n",
    "\n",
    "**<font color=\"red\">Answer options:</font>**\n",
    "1. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(y_k - \\sigma\\left(z_k\\right)\\right)$\n",
    "2. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(\\sigma\\left(z_k\\right) - y_k\\right)$\n",
    "3. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(y_k - \\sigma\\left(z_k\\right)x_m\\right)$\n",
    "4. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(\\sigma\\left(z_k\\right)x_m - y_k\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic model implementation\n",
    "First, let's check the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:14:38.457951Z",
     "start_time": "2018-11-12T21:14:38.450953Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install watermark\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:14:39.327913Z",
     "start_time": "2018-11-12T21:14:38.634925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.6.4\n",
      "IPython 7.1.1\n",
      "\n",
      "numpy 1.14.0\n",
      "scipy 1.0.0\n",
      "pandas 0.22.0\n",
      "matplotlib 2.1.2\n",
      "sklearn 0.19.1\n",
      "\n",
      "compiler   : GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)\n",
      "system     : Darwin\n",
      "release    : 17.7.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "Git hash   : 79a66a97b775db6665881fca0c42e47128480fb0\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -m -p numpy,scipy,pandas,matplotlib,sklearn -g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Docker](https://github.com/Yorko/mlcourse.ai/tree/master/docker_files) and author's laptop configuration**:\n",
    "    \n",
    "CPython 3.5.2<br>\n",
    "IPython 7.0.1\n",
    "\n",
    "numpy 1.15.2<br>\n",
    "scipy 1.1.0<br>\n",
    "pandas 0.23.4<br>\n",
    "matplotlib 3.0.0<br>\n",
    "sklearn 0.20.0\n",
    "\n",
    "compiler   : GCC 5.4.0 20160609<br>\n",
    "system     : Linux<br>\n",
    "release    : 4.17.14-041714-generic<br>\n",
    "machine    : x86_64<br>\n",
    "processor  : x86_64<br>\n",
    "CPU cores  : 12<br>\n",
    "interpreter: 64bit<br>\n",
    "Git hash   : 379461ca2ad94f9ed214dfcc1122f00649852385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:14:40.368913Z",
     "start_time": "2018-11-12T21:14:39.970765Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '{:g}'.format(x))\n",
    "np.set_printoptions(suppress=True)\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and preprocess our [dataset](https://drive.google.com/file/d/1djuygR3cTXb4go_KMWSOw94WlFKb2NT5/view?usp=sharing). Change paths to data files if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:14:40.783902Z",
     "start_time": "2018-11-12T21:14:40.780892Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/stackoverflow_sample_70k.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i ve got some code in window scroll that check...</td>\n",
       "      <td>javascript jquery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have a custom adapter for a list view it has...</td>\n",
       "      <td>android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in my form panel i added a checkbox setting st...</td>\n",
       "      <td>javascript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i have the two dates variables startwork and e...</td>\n",
       "      <td>c#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i might have been using the wrong search tags ...</td>\n",
       "      <td>android</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question               tags\n",
       "0  i ve got some code in window scroll that check...  javascript jquery\n",
       "1  i have a custom adapter for a list view it has...            android\n",
       "2  in my form panel i added a checkbox setting st...         javascript\n",
       "3  i have the two dates variables startwork and e...                 c#\n",
       "4  i might have been using the wrong search tags ...            android"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top tags on StackOverflow for these 70k questions are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:14:41.089827Z",
     "start_time": "2018-11-12T21:14:41.083816Z"
    }
   },
   "outputs": [],
   "source": [
    "top_tags = ['python', 'ios', 'html', 'android', 'c++', 'jquery', 'java', 'php', 'c#', 'javascript']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`question` and `tags` are strings, so we need to preprocess them. \n",
    "\n",
    "Preprocessing steps will be as follows: \n",
    "- convert to lowercase \n",
    "- strip whitespaces \n",
    "- split by whitespaces to form a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:14:48.264230Z",
     "start_time": "2018-11-12T21:14:45.437883Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.94 s, sys: 337 ms, total: 2.27 s\n",
      "Wall time: 2.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['tags'] = df['tags'].str.lower()\\\n",
    "                       .str.strip()\\\n",
    "                       .str.split(' ')\n",
    "df['question'] = df['question'].str.lower()\\\n",
    "                               .str.strip()\\\n",
    "                               .str.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:14:48.284225Z",
     "start_time": "2018-11-12T21:14:48.267218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[i, ve, got, some, code, in, window, scroll, t...</td>\n",
       "      <td>[javascript, jquery]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[i, have, a, custom, adapter, for, a, list, vi...</td>\n",
       "      <td>[android]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[in, my, form, panel, i, added, a, checkbox, s...</td>\n",
       "      <td>[javascript]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[i, have, the, two, dates, variables, startwor...</td>\n",
       "      <td>[c#]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[i, might, have, been, using, the, wrong, sear...</td>\n",
       "      <td>[android]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question                  tags\n",
       "0  [i, ve, got, some, code, in, window, scroll, t...  [javascript, jquery]\n",
       "1  [i, have, a, custom, adapter, for, a, list, vi...             [android]\n",
       "2  [in, my, form, panel, i, added, a, checkbox, s...          [javascript]\n",
       "3  [i, have, the, two, dates, variables, startwor...                  [c#]\n",
       "4  [i, might, have, been, using, the, wrong, sear...             [android]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:14:48.630133Z",
     "start_time": "2018-11-12T21:14:48.581133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70000 entries, 0 to 69999\n",
      "Data columns (total 2 columns):\n",
      "question    70000 non-null object\n",
      "tags        70000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 128.2 MB\n"
     ]
    }
   ],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a template of the class `LogRegressor`, analyze it carefully paying attention to all comments. Then fill in the blanks, run the resulting model and answer the test question.\n",
    "\n",
    "As you might notice, when you update the weight of $ w_ {km} $, the value of the sign $ x_m $ is used, which is $ 0 $ if the word with the index $ m $ is not in the sentence, and is greater than zero if there is such a word. Accordingly, when calculating a linear combination $ z $ of model weights and sample features, only non-zero attributes of the object should be considered.\n",
    "\n",
    "Hint:\n",
    "- If you implement the calculation of the sigmoid in the same way as in the formula, then for a large negative value $ z $ the calculation of $ e ^ {- z} $ turns into a very large number that will go beyond permissible limits\n",
    "- at the same time $ e ^ {- z} $ from a large positive $z$ will be zero\n",
    "- use properties of the sigmoid function $\\sigma$  to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:14:49.354033Z",
     "start_time": "2018-11-12T21:14:49.324043Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    def __init__(self, tags):  \n",
    "        \"\"\"LogRegressor class constructor\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        tags: list of string\n",
    "        \"\"\"\n",
    "        self.__version__ = 'v0.3'\n",
    "        # `set` will drop duplicated tags\n",
    "        self._tags = set(tags)\n",
    "        \n",
    "        # A dictionary that contains the mapping of sentence words and tags into indexes (to save memory)\n",
    "        # example: self._vocab ['exception'] = 17 means that the word \"exception\" has an index of 17\n",
    "        self._vocab = {} #defaultdict(lambda: len(self._vocab))\n",
    "        \n",
    "        # parameters of the model: weights\n",
    "        # for each class / tag we need to store its own vector of weights\n",
    "        # By default, all weights will be zero\n",
    "        # we do not know in advance how many scales we will need\n",
    "        # so for each class we create a dictionary of a variable size with a default value of 0\n",
    "        # example: self._w['java'][self._vocab['exception']] contains weight for word exception and tag java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # parameters of the model: bias term or w_0 weight\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "    \n",
    "    def update_vocab(self, words_list):\n",
    "        \"\"\"Update vocab with new words from words_list\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        words_list: list of strings\n",
    "        \"\"\"\n",
    "        for word in words_list:\n",
    "            # every new word will get index=len(self._vocab)\n",
    "            # so at the end of training all wards will numbered from 0 to len(self._vocab)\n",
    "            if word not in self._vocab:\n",
    "                self._vocab[word] = len(self._vocab)\n",
    "    \n",
    "    def generate_vocab(self, df, column_name):\n",
    "        \"\"\"Build words vocab from dataframe column of lists\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df: pandas.Dataframe\n",
    "        \n",
    "        column_name: string\n",
    "        \"\"\"\n",
    "        if column_name not in df.columns:\n",
    "            raise ValueError(\"DataFrame doesnt have '{}' column!\")\n",
    "        df[column_name].map(self.update_vocab)\n",
    "\n",
    "    def fit_sample(self, sample):\n",
    "        \"\"\"Fit single sample\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample: pandas.Series\n",
    "            dict-like object which contains question and his tags\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas.Series object with metrics for sample\n",
    "        \"\"\"\n",
    "        # sample.name is value from df.index aka row number\n",
    "        sample_id = sample.name\n",
    "        question = sample['question']\n",
    "        tags = set(sample['tags'])\n",
    "        \n",
    "        sample_loss = 0\n",
    "        \n",
    "        # derive the gradients for each tag\n",
    "        for tag in self._tags:\n",
    "            # target is 1 if current sample has current tag \n",
    "            y = int(tag in tags)\n",
    "            # calculate linear combination of weights and features\n",
    "            # HERE'S YOUR CODE\n",
    "            # z = ...\n",
    "            z = sum([self._w[tag][self._vocab[word]] for word in question if word in self._vocab])\n",
    "            \n",
    "            for word in question:\n",
    "                is_word_unknown = word not in self._vocab\n",
    "                # in the test mode, ignore the words that are not in the vocabulary\n",
    "                if sample_id >= self.top_n_train and is_word_unknown:\n",
    "                    continue\n",
    "                # HERE'S YOUR CODE\n",
    "                # z += ...\n",
    "                if is_word_unknown:\n",
    "                    self.update_vocab(word)\n",
    "                    z += self._w[tag][self._vocab[word]]\n",
    "                \n",
    "            # calculate the probability of tag \n",
    "            # HERE'S YOUR CODE\n",
    "            # sigma = ...\n",
    "            sigma = 1 / (1+np.exp(np.clip(-z, -6, 6)))\n",
    "\n",
    "            # update the value of the loss function for the current example\n",
    "            # HERE'S YOUR CODE\n",
    "            # sample_loss += ...\n",
    "            sample_loss += -(y * np.log(sigma) + (1-y)* np.log(1-sigma))\n",
    "\n",
    "            # If still in the training part, update the parameters\n",
    "            if sample_id < self.top_n_train:\n",
    "                # compute the log-likelihood derivative by weight\n",
    "                # HERE'S YOUR CODE\n",
    "                # dLdw = ...\n",
    "                dLdw = -(y - sigma)\n",
    "                # make gradient descent step\n",
    "                # We minimize negative log-likelihood (second minus sign)\n",
    "                # so we go to the opposite direction of the gradient to minimize it (the first minus sign)\n",
    "                delta = self.learning_rate * dLdw\n",
    "                for word in question:                        \n",
    "                    self._w[tag][self._vocab[word]] -= -delta\n",
    "                self._b[tag] -= -delta\n",
    "        if sample_id % self.show_period == 0:\n",
    "            n = sample_id + self.show_period\n",
    "            clear_output(wait=True)\n",
    "            print('LogRegressor {} | {} ({:.2f}%) samples fitted.'.format(\n",
    "                self.__version__,\n",
    "                n, \n",
    "                100 * n / self.total_len))\n",
    "        return pd.Series({'loss': sample_loss})\n",
    "    \n",
    "    def fit_dataframe(self, \n",
    "                      df,\n",
    "                      top_n_train=60000, \n",
    "                      learning_rate=0.1,\n",
    "                      tolerance=1e-16):\n",
    "        \"\"\"One run through dataframe\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            pandas DataFrame with question and tags data\n",
    "\n",
    "        top_n_train : int\n",
    "            first top_n_train samples will be used for training, the rest are for the test\n",
    "            default=60000\n",
    "\n",
    "        learning_rate : float \n",
    "            gradient descent training speed\n",
    "            default=0.1\n",
    "\n",
    "        tolerance : float \n",
    "            used for bounding the values of logarithm argument\n",
    "            default=1e-16\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas.DataFrame with metrics for each sample\n",
    "        \"\"\"\n",
    "        self.total_len = df.shape[0]\n",
    "        self.top_n_train = top_n_train\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tolerance = tolerance\n",
    "        \n",
    "        if self.top_n_train > self.total_len:\n",
    "            print(\"Warning! 'top_n_train' more than dataframe rows count!\\n\"\n",
    "                  \"Set default 'top_n_train'=60000\")\n",
    "            self.top_n_train = 60000\n",
    "        \n",
    "        # generating self._vocab\n",
    "        self.generate_vocab(df, column_name='question')\n",
    "        # Show progress every self.show_period sample, 1% by default\n",
    "        self.show_period = self.total_len // 100\n",
    "        # apply self.fit_sample to each row (sample) of dataframe\n",
    "        self.metrics = df.apply(self.fit_sample, axis=1)\n",
    "        return self.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:17:11.916995Z",
     "start_time": "2018-11-12T21:14:49.651937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogRegressor v0.3 | 70000 (100.00%) samples fitted.\n",
      "CPU times: user 3min 1s, sys: 1.05 s, total: 3min 2s\n",
      "Wall time: 3min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogRegressor(tags=top_tags)\n",
    "# by default, we will train on first 60k samples, and test on last 10k\n",
    "metrics = model.fit_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:17:11.925980Z",
     "start_time": "2018-11-12T21:17:11.918982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60.0248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42.0248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51.9013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42.0248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42.0248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     loss\n",
       "0 60.0248\n",
       "1 42.0248\n",
       "2 51.9013\n",
       "3 42.0248\n",
       "4 42.0248"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the value of negative logarithmic likelihood has actually decreased. Since we are using stochastic gradient descent, we should not expect a smooth fall of the loss function. We will use a moving average with a window of 10,000 examples to smooth the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:17:12.104994Z",
     "start_time": "2018-11-12T21:17:11.928979Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a4d2c6860>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = plt.plot(pd.Series(metrics['loss'][:-10000]).rolling(10000).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spolier** to save your time: if you get such a graph of the loss function, it's OK. \n",
    "\n",
    "<img src='../../img/assignment8_loss.png' width=40%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:17:12.116004Z",
     "start_time": "2018-11-12T21:17:12.107994Z"
    }
   },
   "outputs": [],
   "source": [
    "last_10k_train_loss = np.mean(metrics['loss'][-20000:-10000]) \n",
    "print('Mean of the loss function on the last 10k TRAIN samples: {:.2f}'.format(last_10k_train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">Question 3:</font> What's the average value of the cost function for the last 10000 examples of the training set?**\n",
    "\n",
    "*For discussions, please stick to [ODS Slack](https://opendatascience.slack.com/), channel #mlcourse_ai, pinned thread __#a8_q3__*\n",
    "\n",
    "**<font color=\"red\">Answer options:</font>**\n",
    "1. 18.31\n",
    "2. 19.86\n",
    "3. 21.74\n",
    "4. 26.43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Model testing\n",
    "\n",
    "In the base model, the first 60k rows are used for training, and the remaining ones are used as a test set. As you can see, the value of negative log-likelihood is not very informative, although it allows you to compare different models. In the fourth task, you need to modify the base model so that the `fit_dataframe` method calculate the value of _accuracy_ on the test portion of the dataset for every sample.\n",
    "\n",
    "The accuracy is defined as following:\n",
    "- consider that the question has a tag if the predicted probability of the tag is greater than 0.9\n",
    "- the accuracy of one example is calculated as [Jaccard coefficient](https://en.wikipedia.org/wiki/Jaccard_index) between the set of real tags and tags predicted by the model\n",
    "  - for example, if the example has real tags ['html', 'jquery'], and according to the model they are ['ios', 'html', 'java'], then the Jaccard coefficient will be |['html', 'jquery'] $\\cap$ ['ios', 'html', 'java']| / |['html', 'jquery'] $\\cup$ ['ios', 'html', 'java']| = |['html']| / |['jquery', 'ios', 'html', 'java']| = 1/4\n",
    "- `fit_dataframe` method returns _pd.DataFrame_ with column _Jaccard_ \n",
    "- For answer you need to calculate **average (mean)** accuracy on _Jaccard_ column on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Modified class:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:17:12.135997Z",
     "start_time": "2018-11-12T21:17:12.117991Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    def __init__(self, tags): \n",
    "        self.__version__ = 'v0.4'\n",
    "        self._tags = set(tags)\n",
    "        self._vocab = {}\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "    \n",
    "    def update_vocab(self, words_list):\n",
    "        for word in words_list:\n",
    "            if word not in self._vocab:\n",
    "                self._vocab[word] = len(self._vocab)\n",
    "    \n",
    "    def generate_vocab(self, df, column_name):\n",
    "        if column_name not in df.columns:\n",
    "            raise ValueError(\"DataFrame doesnt have '{}' column!\")\n",
    "        df[column_name].map(self.update_vocab)\n",
    "\n",
    "    def fit_sample(self, sample):\n",
    "        sample_id = sample.name\n",
    "        question = sample['question']\n",
    "        tags = set(sample['tags'])\n",
    "        sample_loss = 0\n",
    "        predicted_tags = None\n",
    "\n",
    "        for tag in self._tags:\n",
    "            y = int(tag in tags)\n",
    "            # HERE'S YOUR CODE\n",
    "            # z = ...\n",
    "\n",
    "            for word in question:\n",
    "                is_word_unknown = word not in self._vocab\n",
    "                if sample_id >= self.top_n_train and is_word_unknown:\n",
    "                    continue\n",
    "                # HERE'S YOUR CODE\n",
    "                # z += ...\n",
    "\n",
    "            # HERE'S YOUR CODE\n",
    "            # sigma = ...\n",
    "            \n",
    "            # HERE'S YOUR CODE\n",
    "            # sample_loss += ...\n",
    "\n",
    "            if sample_id < self.top_n_train:\n",
    "                # HERE'S YOUR CODE\n",
    "                # dLdw = ...\n",
    "\n",
    "                delta = self.learning_rate*dLdw\n",
    "                for word in question:                        \n",
    "                    self._w[tag][self._vocab[word]] -= -delta\n",
    "                self._b[tag] -= -delta\n",
    "            else:\n",
    "                if predicted_tags is None:\n",
    "                    predicted_tags = []\n",
    "                # HERE'S YOUR CODE\n",
    "                # if sigma... :\n",
    "                #     predicted_tags...\n",
    "\n",
    "        if sample_id % self.show_period == 0:\n",
    "            n = sample_id + self.show_period\n",
    "            clear_output(wait=True)\n",
    "            print('LogRegressor {} | {} ({:.2f}%) samples fitted.'.format(\n",
    "                self.__version__,\n",
    "                n, \n",
    "                100 * n / self.total_len))\n",
    "        if predicted_tags is not None:\n",
    "            # HERE'S YOUR CODE\n",
    "            # Jaccard = ...\n",
    "            return pd.Series({'loss': sample_loss, 'Jaccard': Jaccard})\n",
    "        else:\n",
    "            return pd.Series({'loss': sample_loss, 'Jaccard': np.NaN})\n",
    "\n",
    "    \n",
    "    def fit_dataframe(self, \n",
    "                      df,\n",
    "                      top_n_train=60000, \n",
    "                      learning_rate=0.1,\n",
    "                      tolerance=1e-16,\n",
    "                      accuracy_level=0.9):\n",
    "        self.total_len = df.shape[0]\n",
    "        self.top_n_train = top_n_train\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tolerance = tolerance\n",
    "        self.accuracy_level = accuracy_level\n",
    "        \n",
    "        if self.top_n_train > self.total_len:\n",
    "            print(\"Warning! 'top_n_train' more than dataframe rows count!\\n\"\n",
    "                  \"Set default 'top_n_train'=60000\")\n",
    "            self.top_n_train = 60000\n",
    "        \n",
    "        self.generate_vocab(df, column_name='question')\n",
    "        self.show_period = self.total_len // 100\n",
    "        self.metrics = df.apply(self.fit_sample, axis=1)\n",
    "        return self.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:19:34.050653Z",
     "start_time": "2018-11-12T21:17:12.136994Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = LogRegressor(tags=top_tags)\n",
    "metrics = model.fit_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:19:34.060546Z",
     "start_time": "2018-11-12T21:19:34.051636Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:19:34.144200Z",
     "start_time": "2018-11-12T21:19:34.104196Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:19:34.084203Z",
     "start_time": "2018-11-12T21:19:34.062535Z"
    }
   },
   "outputs": [],
   "source": [
    "# HERE'S YOUR CODE\n",
    "# accuracy = ...\n",
    "print('Mean Jaccard accuracy: {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**<font color=\"red\">Question 4:</font> What mean Jaccard accuracy did you get for the test set?**\n",
    "\n",
    "*For discussions, please stick to [ODS Slack](https://opendatascience.slack.com/), channel #mlcourse_ai, pinned thread __#a8_q4__*\n",
    "\n",
    "**<font color=\"red\">Answer options:</font>**\n",
    "1. 0.31\n",
    "2. 0.41\n",
    "3. 0.51\n",
    "4. 0.61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. $L_2$-regularization\n",
    "\n",
    "In the fifth task, you need to add $ L_2 $-regularization to the `LogRegressor` class. In the `fit_sample` method, the` l2_reg = 0.01` parameter with the default value should appear. Taking into account regularization, the new cost function takes the form:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\frac{\\lambda}{2} \\sum_{k=1}^K\\sum_{i=1}^d w_{ki}^2\n",
    "\\end{array}$$\n",
    "\n",
    "We have already derived the gradient of the first term of the sum, and for the second one it looks like:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial}{\\partial w_{ki}} \\frac{\\lambda}{2} R\\left(W\\right) &=& \\lambda w_{ki}\n",
    "\\end{array}$$\n",
    "\n",
    "If we make an explicit update of all weights on each example, then the process will be very slow, because we have to run through every word of the dictionary at each iteration. At the expense of the theoretical accuracy, we use a dirty trick: we will regularize only those words that are present in the current sentence. Do not forget that the bias term is not regularized. `sample_loss` should also remain unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Modified class:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:19:34.102197Z",
     "start_time": "2018-11-12T21:19:34.085202Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    def __init__(self, tags): \n",
    "        self.__version__ = 'v0.5'\n",
    "        self._tags = set(tags)\n",
    "        self._vocab = {}\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "    \n",
    "    def update_vocab(self, words_list):\n",
    "        for word in words_list:\n",
    "            if word not in self._vocab:\n",
    "                self._vocab[word] = len(self._vocab)\n",
    "    \n",
    "    def generate_vocab(self, df, column_name):\n",
    "        if column_name not in df.columns:\n",
    "            raise ValueError(\"DataFrame doesnt have '{}' column!\")\n",
    "        df[column_name].map(self.update_vocab)\n",
    "\n",
    "    def fit_sample(self, sample):\n",
    "        sample_id = sample.name\n",
    "        question = sample['question']\n",
    "        tags = set(sample['tags'])\n",
    "        sample_loss = 0\n",
    "        predicted_tags = None\n",
    "\n",
    "        for tag in self._tags:\n",
    "            y = int(tag in tags)\n",
    "            # HERE'S YOUR CODE\n",
    "            # z = ...\n",
    "\n",
    "            for word in question:\n",
    "                is_word_unknown = word not in self._vocab\n",
    "                if sample_id >= self.top_n_train and is_word_unknown:\n",
    "                    continue\n",
    "                # HERE'S YOUR CODE\n",
    "                # z += ...\n",
    "            \n",
    "            # HERE'S YOUR CODE\n",
    "            # sigma = ...\n",
    "            \n",
    "            # HERE'S YOUR CODE\n",
    "            # sample_loss += ...\n",
    "\n",
    "            if sample_id < self.top_n_train:\n",
    "                # HERE'S YOUR CODE\n",
    "                # dLdw = ...\n",
    "\n",
    "                delta = self.learning_rate*dLdw\n",
    "                for word in question:\n",
    "                    # HERE'S YOUR CODE\n",
    "                    # self._w[tag][self._vocab[word]] -= (- delta...\n",
    "                self._b[tag] -= -delta\n",
    "            else:\n",
    "                if predicted_tags is None:\n",
    "                    predicted_tags = []\n",
    "                # HERE'S YOUR CODE\n",
    "                # if sigma... :\n",
    "                #     predicted_tags...\n",
    "\n",
    "        if sample_id % self.show_period == 0:\n",
    "            n = sample_id + self.show_period\n",
    "            clear_output(wait=True)\n",
    "            print('LogRegressor {} | {} ({:.2f}%) samples fitted.'.format(\n",
    "                self.__version__,\n",
    "                n, \n",
    "                100 * n / self.total_len))\n",
    "        if predicted_tags is not None:\n",
    "            # HERE'S YOUR CODE\n",
    "            # Jaccard = ...\n",
    "            return pd.Series({'loss': sample_loss, 'Jaccard': Jaccard})\n",
    "        else:\n",
    "            return pd.Series({'loss': sample_loss, 'Jaccard': np.NaN})\n",
    "\n",
    "    \n",
    "    def fit_dataframe(self, \n",
    "                      df,\n",
    "                      top_n_train=60000, \n",
    "                      learning_rate=0.1,\n",
    "                      tolerance=1e-16,\n",
    "                      accuracy_level=0.9,\n",
    "                      l2_reg=0.01):\n",
    "        self.total_len = df.shape[0]\n",
    "        self.top_n_train = top_n_train\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tolerance = tolerance\n",
    "        self.accuracy_level = accuracy_level\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        if self.top_n_train > self.total_len:\n",
    "            print(\"Warning! 'top_n_train' more than dataframe rows count!\\n\"\n",
    "                  \"Set default 'top_n_train'=60000\")\n",
    "            self.top_n_train = 60000\n",
    "        \n",
    "        self.generate_vocab(df, column_name='question')\n",
    "        self.show_period = self.total_len // 100\n",
    "        self.metrics = df.apply(self.fit_sample, axis=1)\n",
    "        return self.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:22:47.257582Z",
     "start_time": "2018-11-12T21:19:34.149186Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = LogRegressor(tags=top_tags)\n",
    "metrics = model.fit_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:22:47.407541Z",
     "start_time": "2018-11-12T21:22:47.259567Z"
    }
   },
   "outputs": [],
   "source": [
    "# HERE'S YOUR CODE\n",
    "# accuracy = ...\n",
    "print('{:.2f}'.format(accuracy))\n",
    "plot = plt.plot(pd.Series(metrics['loss'][:-10000]).rolling(10000).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">Question 5:</font> What's the average value of Jaccard accuracy in case of $L_2$-regularization?**\n",
    "\n",
    "*For discussions, please stick to [ODS Slack](https://opendatascience.slack.com/), channel #mlcourse_ai, pinned thread __#a8_q5__*\n",
    "\n",
    "**<font color=\"red\">Answer options:</font>**\n",
    "1. 0.32\n",
    "2. 0.38\n",
    "3. 0.48\n",
    "4. 0.52 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ElasticNet regularization, derivation\n",
    "In addition to $ L_2 $ regularization, $ L_1 $ regularization is often used.\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\sum_{k=1}^K\\sum_{i=1}^d \\left|w_{ki}\\right|\n",
    "\\end{array}$$\n",
    "\n",
    "If we linearly combine $ L_1 $ and $ L_2 $ regularization, then the resulting regularization type is called **ElasticNet**:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\lambda R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\left(\\gamma \\sum_{k=1}^K\\sum_{i=1}^d w_{ki}^2 + \\left(1 - \\gamma\\right) \\sum_{k=1}^K\\sum_{i=1}^d \\left|w_{ki}\\right| \\right)\n",
    "\\end{array}$$\n",
    "- where $\\gamma \\in \\left[0, 1\\right]$\n",
    "\n",
    "**<font color=\"red\">Question 6:</font> What's the correct formula for the gradient of the ElasticNet regularization term?**\n",
    "\n",
    "*For discussions, please stick to [ODS Slack](https://opendatascience.slack.com/), channel #mlcourse_ai, pinned thread __#a8_q6__*\n",
    "\n",
    "**<font color=\"red\">Answer options:</font>**\n",
    "1. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(\\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "2. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) w_{ki}\\right)$ \n",
    "3. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "4. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma \\left|w_{ki}\\right| + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ElasticNet regularization, implementation\n",
    "\n",
    "In the seventh task you are supposed to change the class `LogRegressor` so that the` fit_dataframe` method takes two parameters with default values `l2_reg = 0.001` and `l1_reg = 0.1`. Do one pass through the dataset with ElasticNet regularization and default parameter values and answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Modified class:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:39:10.815508Z",
     "start_time": "2018-11-12T21:39:10.797514Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    def __init__(self, tags): \n",
    "        self.__version__ = 'v0.9'\n",
    "        self._tags = set(tags)\n",
    "        self._vocab = {}\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        self._word_stats = defaultdict(int)\n",
    "    \n",
    "    # HERE'S YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:45:33.329289Z",
     "start_time": "2018-11-12T21:39:11.641795Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = LogRegressor(tags=top_tags)\n",
    "metrics = model.fit_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:45:33.498137Z",
     "start_time": "2018-11-12T21:45:33.331289Z"
    }
   },
   "outputs": [],
   "source": [
    "# HERE'S YOUR CODE\n",
    "# accuracy = ...\n",
    "plot = plt.plot(pd.Series(metrics['loss'][:-10000]).rolling(10000).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**<font color=\"red\">Question 7:</font> What's the average value of Jaccard accuracy in case of ElasticNet regularization?**\n",
    "\n",
    "*For discussions, please stick to [ODS Slack](https://opendatascience.slack.com/), channel #mlcourse_ai, pinned thread __#a8_q7__*\n",
    "\n",
    "**<font color=\"red\">Answer options:</font>**\n",
    "1. 0.51\n",
    "2. 0.61\n",
    "3. 0.71\n",
    "4. 0.81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The most important words for a tag\n",
    "\n",
    "The beauty of linear models is that they are somewhat interpretable. You are supposed to calculate which words contribute the most to the probability of each of the tags. And then answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:45:36.839055Z",
     "start_time": "2018-11-12T21:45:33.501141Z"
    }
   },
   "outputs": [],
   "source": [
    "model._vocab_inv = dict([(v, k) for (k, v) in model._vocab.items()])\n",
    "top = 5\n",
    "for tag in model._tags:\n",
    "    # HERE'S YOUR CODE\n",
    "    # top5_words = ...\n",
    "    print(tag, ':', ', '.join(top5_words))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many tags, the presence of the tag itself in the sentence is an important signal, and for many, the tag itself is the strongest signal, which is not surprising. \n",
    "\n",
    "**<font color=\"red\">Question 8:</font> For which of the tags the tag name itself is not included in the top 5 most important words?**\n",
    "\n",
    "*For discussions, please stick to [ODS Slack](https://opendatascience.slack.com/), channel #mlcourse_ai, pinned thread __#a8_q8__*\n",
    "\n",
    "**<font color=\"red\">Answer options:</font>**\n",
    "1. android\n",
    "2. javascript\n",
    "3. jquery\n",
    "4. c#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Reducing the size of the dictionary\n",
    "\n",
    "Now the number of words in the dictionary is too big. If it was a sample of 10 million questions from the StackOverflow website, then the dictionary size would've been ~ 10 million as well. You can regularize the model not only mathematically, but also simply limiting the size of the dictionary. You are supposed to make the following changes in the class `LogRegressor`:\n",
    "- add `self._word_stats = defaultdict(int)` to `__init__` to calculate word frequencies\n",
    "- add one more argument to the `fit_dataframe` method with the default value `freeze_vocab = False`\n",
    "- when `freeze_vocab = False` allow to add words to the dictionary and word_stats\n",
    "- when `freeze_vocab = True` ignore words not from the dictionary and don't update word_stats\n",
    "- add the class method `filter_vocab (n = 10000)`, which will leave only top-n most popular words in the dictionary\n",
    "\n",
    "For first `fit_dataframe` call use `learning_rate=0.2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Modified class:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:45:36.879043Z",
     "start_time": "2018-11-12T21:45:36.844053Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    def __init__(self, tags): \n",
    "        self.__version__ = 'v0.9'\n",
    "        self._tags = set(tags)\n",
    "        self._vocab = {}\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        self._word_stats = defaultdict(int)\n",
    "    \n",
    "    # HERE'S YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:52:34.210452Z",
     "start_time": "2018-11-12T21:45:36.881041Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = LogRegressor(tags=top_tags)\n",
    "# HERE'S YOUR CODE\n",
    "# metrics = model.fit_dataframe(df, ...\n",
    "# HERE'S YOUR CODE\n",
    "# accuracy = ...\n",
    "print('Mean Jaccard accuracy: {:.2f}'.format(accuracy))\n",
    "plot = plt.plot(pd.Series(metrics['loss'][:-10000]).rolling(10000).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leave only 10 000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:52:35.099786Z",
     "start_time": "2018-11-12T21:52:34.211441Z"
    }
   },
   "outputs": [],
   "source": [
    "model.filter_vocab(top_n=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do one more iteration through the dataset, reducing learning rate 20 times and L2-regularization 5 times with freezed vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:58:00.298402Z",
     "start_time": "2018-11-12T21:52:35.100786Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# HERE'S YOUR CODE\n",
    "# metrics = model.fit_dataframe(df, ...\n",
    "# HERE'S YOUR CODE\n",
    "# accuracy = ...\n",
    "print('Mean Jaccard accuracy: {:.2f}'.format(accuracy))\n",
    "plot = plt.plot(pd.Series(metrics['loss'][:-10000]).rolling(10000).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**<font color=\"red\">Question 9:</font> What's the average value of Jaccard accuracy in case of reducing the dictionary size?**\n",
    "\n",
    "*For discussions, please stick to [ODS Slack](https://opendatascience.slack.com/), channel #mlcourse_ai, pinned thread __#a8_q9__*\n",
    "\n",
    "**<font color=\"red\">Answer options:</font>**\n",
    "1. 0.66\n",
    "2. 0.69\n",
    "3. 0.72 \n",
    "4. 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Predicting tags for new questions\n",
    "\n",
    "At the end of this assignment, you are supposed to implement the method `predict_proba`, which takes a model and a string containing the question and returns a list of predicted question tags with their probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:58:40.677323Z",
     "start_time": "2018-11-12T21:58:40.671312Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_proba(model, sentence):\n",
    "    p = {}\n",
    "    # HERE'S YOUR CODE\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:58:41.226458Z",
     "start_time": "2018-11-12T21:58:41.223459Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = (\"I want to improve my coding skills, so I have planned write \" +\n",
    "            \"a Mobile Application.need to choose between Apple's iOS or Google's Android.\" +\n",
    "            \" my background: I have done basic programming in .Net,C/C++,Python and PHP \" +\n",
    "            \"in college, so got OOP concepts covered. about my skill level, I just know \" +\n",
    "            \"concepts and basic syntax. But can't write complex applications, if asked :(\" +\n",
    "            \" So decided to hone my skills, And I wanted to know which is easier to \" +\n",
    "            \"learn for a programming n00b. A) iOS which uses Objective C B) Android \" + \n",
    "            \"which uses Java. I want to decide based on difficulty level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of the question (sentence) will only include converting it to lower case and deleting commas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:58:42.315626Z",
     "start_time": "2018-11-12T21:58:42.311628Z"
    }
   },
   "outputs": [],
   "source": [
    "pred = predict_proba(model, sentence.lower().replace(',', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:58:42.963020Z",
     "start_time": "2018-11-12T21:58:42.960009Z"
    }
   },
   "outputs": [],
   "source": [
    "tag_preds = sorted(pred.items(), key=lambda t: t[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T21:58:43.381228Z",
     "start_time": "2018-11-12T21:58:43.376229Z"
    }
   },
   "outputs": [],
   "source": [
    "list(filter(lambda t: t[1] > 0.9, tag_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">Question 10:</font> Which tag or tags are associated with this question if the acceptance threshold is $ 0.9 $?**\n",
    "\n",
    "*For discussions, please stick to [ODS Slack](https://opendatascience.slack.com/), channel #mlcourse_ai, pinned thread __#a8_q10__*\n",
    "\n",
    "**<font color=\"red\">Answer options:</font>**\n",
    "1. ios\n",
    "2. android\n",
    "3. c#, c++\n",
    "4. ios, php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS: in the original question the following four tags are put: java, android, objective-c, ios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
